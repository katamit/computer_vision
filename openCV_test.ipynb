{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1c478ecf6251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#img = cv.imread(path,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWINDOW_NORMAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 25 16:23:21 2018\n",
    "\n",
    "@author: amitkumar_kataria\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame([12,3,4],[12,3,4])\n",
    "\n",
    "# import cv2\n",
    "\n",
    "\n",
    "##----- testing the class and private member functinality  in a class\n",
    "#\n",
    "#class myClass:\n",
    "#    cls_var = 10\n",
    "#    def __init__(self, name, game):\n",
    "#        self.name = name\n",
    "##        self.game = game\n",
    "#        self.cls_var = game\n",
    "#    @staticmethod    \n",
    "#    def __provd():\n",
    "#        print(' i ma teh provdferer ')\n",
    "#        \n",
    "#    def obj_method(self):\n",
    "#        print(' callin theh provarte method ')\n",
    "##        print(self.cls_var)\n",
    "#        myClass.__provd()\n",
    "#        \n",
    "#fir = myClass('one', 1)\n",
    "#sec = myClass('one', 2)\n",
    "#\n",
    "### - By default all the user defined objects are hashable\n",
    "##print(hash(fir))\n",
    "##print(hash(sec))\n",
    "##\n",
    "##obj_set = set([fir,sec])\n",
    "##print(obj_set)\n",
    "### ---- hash ends ----->>>>>>>>>>\n",
    "#\n",
    "#print( fir.name)\n",
    "#print(fir.cls_var)\n",
    "#print(myClass.cls_var)\n",
    "#myClass._myClass__provd()\n",
    "#myClass.obj_method('fdfdf' )\n",
    "#fir.obj_method()\n",
    "\n",
    "        \n",
    "# ---------------  Ends here\n",
    "        \n",
    "\n",
    "## ----------------------- open the image using opencv--------\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "#import os \n",
    "\n",
    "#path = os.path('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\4.jpg')\n",
    "\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\1.jpg',-1)\n",
    "#img = cv.imread(path,0)\n",
    "print(img)\n",
    "print(img.shape)\n",
    "print(type(img))\n",
    "cv.namedWindow('image', cv.WINDOW_NORMAL)\n",
    "cv.imshow('image', img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "#---------------- capturing video with opencv--------\n",
    "cap = cv.VideoCapture()\n",
    "#while True:\n",
    "#    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "#-------------- DRAWING ITHE SHAPES ----------\n",
    "\n",
    "# Create a black image\n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "# Draw a diagonal blue line with thickness of 5 px\n",
    "cv.line(img,(0,0),(511,511),(255,0,0),5)\n",
    "cv.rectangle(img,(384,0),(510,128),(0,255,0),3)\n",
    "cv.circle(img,(447,63), 63, (0,0,255), 0)\n",
    "cv.ellipse(img,(256,256),(50,50),100,10,330,100,20)\n",
    "pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)\n",
    "pts = pts.reshape((-1,1,2))\n",
    "cv.polylines(img,[pts],False,(0,255,255))\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "cv.putText(img,'OpenCV',(10,500), font, 2,(255,255,255),2,cv.LINE_AA)\n",
    "\n",
    "cv.circle(img,(200,200), 30, (0,0,255), 0)\n",
    "cv.circle(img,(180,263), 30, (0,255,0), 0)\n",
    "cv.circle(img,(250,263), 30, (255,0,0), 0)\n",
    "\n",
    "\n",
    "##---------  opencv ends >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II GUI Features in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------  dRAWING IMAGES ON MOUSE EVENT ----------\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "# mouse callback function\n",
    "def draw_circle(event,x,y,flags,param):\n",
    "    if event == cv.EVENT_LBUTTONDBLCLK:\n",
    "        cv.circle(img,(x,y),100,(255,0,0),-1)\n",
    "# Create a black image, a window and bind the function to window\n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "cv.namedWindow('image')\n",
    "cv.setMouseCallback('image',draw_circle)\n",
    "while(1):\n",
    "    cv.imshow('image',img)\n",
    "    if cv.waitKey(20) & 0xFF == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#------------  mouse callback setup - second example-----\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "drawing = False # true if mouse is pressed\n",
    "mode = True # if True, draw rectangle. Press 'm' to toggle to curve\n",
    "ix,iy = -1,-1\n",
    "# mouse callback function\n",
    "def draw_circle(event,x,y,flags,param):\n",
    "    global ix,iy,drawing,mode\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix,iy = x,y\n",
    "    elif event == cv.EVENT_MOUSEMOVE:\n",
    "        if drawing == True:\n",
    "            if mode == True:\n",
    "                cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n",
    "            else:\n",
    "                cv.circle(img,(x,y),5,(0,0,255),-1)\n",
    "    elif event == cv.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        if mode == True:\n",
    "            cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\n",
    "        else:\n",
    "            cv.circle(img,(x,y),5,(0,0,255),-1)\n",
    "            \n",
    "img = np.zeros((512,512,3), np.uint8)\n",
    "cv.namedWindow('image')\n",
    "cv.setMouseCallback('image',draw_circle)\n",
    "while(1):\n",
    "    cv.imshow('image',img)\n",
    "    k = cv.waitKey(1) & 0xFF\n",
    "    if k == ord('m'):\n",
    "        mode = not mode\n",
    "    elif k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#displaying the video caputure using the webcam\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while(True):\n",
    "# Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "# Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "# Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    " \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#playing a viedo from file\n",
    "\n",
    "cap = cv2.VideoCapture('/home/amit/Videos/OOPAR OOPAR RENN DE _ The Bro Anthem _ Happii-Fi-O9qaUEaKUeU.webm')\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Capturing and saving a fliped (upside-Down) video using webcam\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        frame = cv2.flip(frame,0)\n",
    "# write the flipped frame\n",
    "        out.write(frame)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracker example with the provide assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trackbar example\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "def nothing(x):\n",
    "    pass \n",
    "\n",
    "b = 255\n",
    "g = 0\n",
    "r = 0\n",
    "# radius = 10\n",
    "\n",
    "def draw_circle(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK:\n",
    "        cv2.circle(img,(244,151),radius,(b,g,r),-1)\n",
    "# Create a black image, a window\n",
    "img = np.zeros((300,512,3), np.uint8)\n",
    "cv2.namedWindow('image')\n",
    "cv2.setMouseCallback('image',draw_circle)\n",
    "\n",
    "\n",
    "# create trackbars for color change\n",
    "cv2.createTrackbar('R','image',0,255,nothing)\n",
    "cv2.createTrackbar('G','image',0,255,nothing)\n",
    "cv2.createTrackbar('B','image',0,255,nothing)\n",
    " \n",
    "# create switch for ON/OFF functionality\n",
    "switch = '0 : OFF \\n1 : ON'\n",
    "cv2.createTrackbar(switch, 'image',0,1,nothing)\n",
    "cv2.createTrackbar('Radius', 'image',10,100,nothing)\n",
    "\n",
    "\n",
    "while(1):\n",
    "    cv2.imshow('image',img)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# get current positions of four trackbars\n",
    "    r = cv2.getTrackbarPos('R','image')\n",
    "    g = cv2.getTrackbarPos('G','image')\n",
    "    b = cv2.getTrackbarPos('B','image')\n",
    "    radius = cv2.getTrackbarPos('Radius','image')\n",
    "    \n",
    "    s = cv2.getTrackbarPos(switch,'image')\n",
    " \n",
    "    if s == 0:\n",
    "        img[:] = 0\n",
    "#     else:\n",
    "#         img[:] = [b,g,r]\n",
    "#         cv2.setMouseCallback('image',draw_circle)\n",
    "\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III   CORE OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 3.1 BASIC OPERATION ON IMAGES---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "messi = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',-1)\n",
    "print(messi.shape)\n",
    "px = messi[100,100]\n",
    "print(px)\n",
    "\n",
    "#accessing the blue Pixel\n",
    "blue = messi[...,0]\n",
    "green = messi[...,1]\n",
    "red = messi[...,2]\n",
    "\n",
    "print(blue.shape)\n",
    "messi[193,258] = [0,0,0]\n",
    "cv.imshow('blue_pixels', messi)\n",
    "#cv.imshow('read_messi', messi)\n",
    "cv.waitKey(000)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "print(messi.item((10,10,2)))\n",
    "messi.itemset((10,10,2),0)\n",
    "print(messi.item((10,10,2)))\n",
    "\n",
    "# Accessing Image Properties\n",
    "print(messi.ndim)  # same as print(len(pic.shape))\n",
    "print(messi.shape)\n",
    "print(messi.size)\n",
    "print(messi.dtype)\n",
    "\n",
    "# Image ROI\n",
    "'''\n",
    "you will have to play with certain region of images\n",
    "'''\n",
    "# REplaceing the second ball with grass\n",
    "#ball = messi[210:270,330:390]\n",
    "#print(ball.shape)\n",
    "#print(ball[0:1,...].shape)\n",
    "#messi[273:333, 100:160]= ball[0:1,...]\n",
    "#print(messi[280:340,330:390].shape)\n",
    "cv.circle(messi,(103,240),30,(18,63,25),-1)\n",
    "#print(tuple(messi[190,240]))\n",
    "cv.imshow('read_messi', messi)\n",
    "cv.waitKey(000)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Making Borders for Images (Padding)\n",
    "'''\n",
    "If you want to create a border around the image, something like a photo frame,\n",
    "you can use cv.copyMakeBorder(). But it has more applications for convolution \n",
    "operation, zero padding etc.\n",
    "'''\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "BLUE = [255,0,0]\n",
    "RED = [255,0,0]\n",
    "WHITE = [255,255,255]\n",
    "img1 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\2.jpg',-1)\n",
    "replicate = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REPLICATE)\n",
    "reflect = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT)\n",
    "reflect101 = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT_101)\n",
    "wrap = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_WRAP)\n",
    "constant= cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_CONSTANT,value=WHITE)\n",
    "plt.subplot(231),plt.imshow(img1,'gray'),plt.title('ORIGINAL')\n",
    "plt.subplot(232),plt.imshow(replicate,'gray'),plt.title('REPLICATE')\n",
    "plt.subplot(233),plt.imshow(reflect,'gray'),plt.title('REFLECT')\n",
    "plt.subplot(234),plt.imshow(reflect101,'gray'),plt.title('REFLECT_101')\n",
    "plt.subplot(235),plt.imshow(wrap,'gray'),plt.title('WRAP')\n",
    "plt.subplot(236),plt.imshow(constant,'gray'),plt.title('CONSTANT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Arithmetic Operations on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    " cv.add(), cv.addWeighted() etc\n",
    "'''\n",
    "\n",
    "x = np.uint8([250])\n",
    "y = np.uint8([10])\n",
    "print(cv.add(x,y))  # 250+10 = 260 ==> 255\n",
    "print(x+y)  # 250+10 = 260 ==> 4\n",
    "\n",
    "# Image Blending\n",
    "img1  = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',-1)\n",
    "img2  = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\opencv_logo.png',-1)\n",
    "\n",
    "print(img1.shape)\n",
    "print(img2.shape)\n",
    "\n",
    "## Observe the difference between cv.add and numpy add of the pixels\n",
    "img3 = img1[26:250,60:284,]+img2\n",
    "cv.imshow(\"Blended image\" , img3)\n",
    "cv.waitKey(5000);\n",
    "cv.destroyAllWindows()\n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Blend_open_messi_image.jpg',img3)\n",
    "\n",
    "img4 = cv.add(img1[26:250,60:284,],img2)\n",
    "cv.imshow(\"Blended image\" , img4)\n",
    "cv.waitKey(5000);\n",
    "cv.destroyAllWindows()\n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Blend_open_messi.jpg',img4)\n",
    "\n",
    "## Image Bleinging\n",
    "img4 = cv.addWeighted(img1[26:250,60:284,],0.8,img2,0.3,-10)\n",
    "cv.imshow(\"Blended image\" , img4)\n",
    "cv.waitKey(10000);\n",
    "cv.destroyAllWindows()\n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Blend_open_messi_blended.jpg',img4)\n",
    "\n",
    "## Bitwise Operations\n",
    "'''\n",
    "This includes bitwise AND, OR, NOT and XOR operations.\n",
    " They will be highly useful while extracting any part of the image\n",
    " (as we will see in coming chapters), \n",
    " defining and working with non-rectangular ROI\n",
    "'''\n",
    "# Load two images\n",
    "messi = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "logo = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\opencv_logo.png')\n",
    "#logo = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\opencv_logo_black.png')\n",
    "# I want to put logo on top-left corner, So I create a ROI\n",
    "rows,cols,channels = logo.shape\n",
    "roi = messi[0:rows, 0:cols ]\n",
    "# Now create a mask of logo and create its inverse mask also\n",
    "logo2gray = cv.cvtColor(logo,cv.COLOR_BGR2GRAY)\n",
    "#print(logo2gray)\n",
    "ret, mask = cv.threshold(logo2gray, 150,maxval=255, type=cv.THRESH_BINARY)\n",
    "#print(ret)\n",
    "mask_inv = cv.bitwise_not(mask)\n",
    "# Now black-out the area of logo in ROI\n",
    "img1_bg = cv.bitwise_and(roi,roi,mask = mask)\n",
    "#messi[0:rows, 0:cols ] = img1_bg\n",
    "# Take only region of logo from logo image.\n",
    "logo2_fg = cv.bitwise_and(logo,logo,mask = mask_inv)\n",
    "#cv.imshow('blackout',logo2_fg)\n",
    "#cv.waitKey(0)\n",
    "#cv.destroyAllWindows()\n",
    "# Put logo in ROI and modify the main image\n",
    "dst = cv.add(img1_bg,logo2_fg)\n",
    "messi[0:rows, 0:cols ] = dst\n",
    "cv.imshow('res',messi)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "## Assignment\n",
    "img1  = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',-1)\n",
    "img2  = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\opencv_logo_black.png',-1)\n",
    "\n",
    "\n",
    "for percent in range(10,100,10):\n",
    "    print(percent/100)\n",
    "    image = cv.addWeighted(img1[26:225,60:260,], percent/100, img2, 1-percent/10,0)\n",
    "    cv.imshow('res',image)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "    cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\slides\\\\{0}.jpg'.format(str(percent//10)),image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV Image Processing in OpenCv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Changing Colorspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "'''\n",
    "Flags for color converion in OpenCV\n",
    "'''\n",
    "flags = [i for i in dir(cv) if i.startswith('COLOR_')]\n",
    "print(len(flags))\n",
    "\n",
    "'''\n",
    "This is the simplest method in object tracking. \n",
    "Once you learn functions of contours, you can do plenty of things \n",
    "like find centroid of this object and use it to track the object, \n",
    "draw diagrams just by moving your hand in front of camera and \n",
    "many other funny stuffs.\n",
    "'''\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "cap = cv.VideoCapture(0)\n",
    "while(1):\n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "    # Convert BGR to HSV\n",
    "    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "    # define range of blue color in HSV\n",
    "    lower_blue = np.array([110,50,50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv.inRange(hsv, lower_blue, upper_blue)\n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv.bitwise_and(frame,frame, mask= mask)\n",
    "    cv.imshow('frame',frame)\n",
    "    cv.imshow('mask',mask)\n",
    "    cv.imshow('res',res)\n",
    "    k = cv.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "green = np.uint8([[[0,255,0 ]]])\n",
    "hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)\n",
    "print( hsv_green )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4.2  Geometric Transformations of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scaling - resize() shirnk or englarge the image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img  = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',-1)\n",
    "res = cv.resize(img,None,fx=4, fy=4, interpolation = cv.INTER_CUBIC)\n",
    "cv.imshow('result',res)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "#OR\n",
    "height, width, _ = img.shape\n",
    "# when the size of output image is provide fx and fy are calculated from there\n",
    "res = cv.resize(img,None,fx=1.4, fy=0.8, interpolation = cv.INTER_AREA)\n",
    "#res = cv.resize(img,None, cv.Size(),0.5,0.5, interpolation = cv.INTER_AREA)\n",
    "cv.imshow('result',res)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# tRANSLATION cv.wrapAffine() takes 2x3 matrix cv.wrapPerspective takes 3x3 matrix\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "\n",
    "rows,cols = img.shape\n",
    "\n",
    "#creating the matrix for inputted to be wrapAffine()\n",
    "M = np.float32([[1,0,100],[0,1,50]])\n",
    "\n",
    "# last argumnent for wrapAfine is size of destination image in widhtxheight\n",
    "dst = cv.warpAffine(img,M,(cols,rows))\n",
    "cv.imshow('img',dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Rotation with 90 degrees and no scaling\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = cv.getRotationMatrix2D(center= (cols/2,rows/2), angle= 270,scale=2)\n",
    "#M = cv.getRotationMatrix2D((cols/2,rows/2),90,1)\n",
    "dest = cv.warpAffine(src=img,M=M,dsize=(rows*2,cols))\n",
    "dst = cv.warpAffine(dest,np.float32([[1,0,100],[0,1,10]]),dsize=(rows*2,cols))\n",
    "cv.imshow('Result',dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4.3 Image Thresholding : cv.threshold, cv.adaptiveThreshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\gradient.jpg',0)\n",
    "ret,thresh1 = cv.threshold(img,200,255,cv.THRESH_BINARY)\n",
    "ret,thresh2 = cv.threshold(img,200,255,cv.THRESH_BINARY_INV)\n",
    "ret,thresh3 = cv.threshold(img,200,255,cv.THRESH_TRUNC)\n",
    "ret,thresh4 = cv.threshold(img,200,255,cv.THRESH_TOZERO)\n",
    "ret,thresh5 = cv.threshold(img,200,255,cv.THRESH_TOZERO_INV)\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1),plt.imshow(images[i], cmap='cool')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Adaptive thresholding \n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\sudoku.jpg',0)\n",
    "#img = cv.resize(img,None, fx=0.5,fy=0.5, interpolation = cv.INTER_AREA)\n",
    "#img = cv.medianBlur(img,5)\n",
    "cv.imshow('Result',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\sudoku.jpg',img)\n",
    "ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY,11,2)\n",
    "th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY,11,2)\n",
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# OTsu's Binarization\n",
    "\n",
    "img = np.zeros((400,400,3), np.uint8)\n",
    "cv.rectangle(img,(200,100),(300,200),(255,255,255),100)\n",
    "cv.imshow('Result',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\noisy2.png',0)\n",
    "# global thresholding\n",
    "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# implementation of OSTu's Binarization\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\noisy2.png',0)\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "# find normalized_histogram, and its cumulative distribution function\n",
    "hist = cv.calcHist([blur],[0],None,[256],[0,256])\n",
    "hist_norm = hist.ravel()/hist.max()\n",
    "Q = hist_norm.cumsum()\n",
    "bins = np.arange(256)\n",
    "fn_min = np.inf\n",
    "thresh = -1\n",
    "for i in range(1,256):\n",
    "    p1,p2 = np.hsplit(hist_norm,[i]) # probabilities\n",
    "    q1,q2 = Q[i],Q[255]-Q[i] # cum sum of classes\n",
    "    b1,b2 = np.hsplit(bins,[i]) # weights\n",
    "    # finding means and variances\n",
    "    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2\n",
    "    v1,v2 = np.sum(((b1-m1)**2)*p1)/q1,np.sum(((b2-m2)**2)*p2)/q2\n",
    "    # calculates the minimization function\n",
    "    fn = v1*q1 + v2*q2\n",
    "    if fn < fn_min:\n",
    "        fn_min = fn\n",
    "        thresh = i\n",
    "# find otsu's threshold value with OpenCV function\n",
    "ret, otsu = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "print( \"{} {}\".format(thresh,ret) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Smoothing Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Blur the images with various low pass filters\n",
    "Apply custom made filters to images\n",
    "\n",
    "Images can also be filtered with various low-pass filters(LPF), \n",
    "high-pass filters (HPF) etc. \n",
    "\n",
    "LPF helps in removing noises,  bluring the images etc\n",
    "HPF helps in finding edges in images.\n",
    "'''\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\opencv_logo_black.png')\n",
    "kernel = np.ones((5,5), np.float32)/25\n",
    "# obser the bluring with different size of the filters\n",
    "#kernel = np.ones((5,6), np.float32)/30\n",
    "#kernel = np.ones((10,10), np.float32)/100\n",
    "#dst = cv.filter2D(img, ddepth=-1, kernel=kernel)\n",
    "dst = cv.filter2D(img, -1, kernel)\n",
    "plt.subplot(121); plt.imshow(img), plt.title('Original')\n",
    "plt.xticks([]);plt.yticks([])\n",
    "plt.subplot(122);plt.imshow(dst);plt.title('Averaging kerneled')\n",
    "plt.xticks([]);plt.yticks([])\n",
    "\n",
    "'''\n",
    "Image blurring is achieved by convolving the image with a low-pass filter \n",
    "kernel. It is useful for removing noises. It actually removes high frequency \n",
    "content (eg: noise, edges) from the image. So edges are blurred a little bit \n",
    "in this operation.\n",
    "\n",
    "OpenCV provides mainly four types of blurring techniques\n",
    "Average Bluring      cv.blur() or cv.boxFilter()\n",
    "Gaussian Bluring     cv.GaussianBlur(img, (x,y),0)\n",
    "Medina Bluring       cv.medianBlur(img, 5)\n",
    "Bilateral Bluring    cv.bilateralFilter(img,9,75,75)\n",
    "'''\n",
    "#img = cv.imread('opencv-logo-white.png')\n",
    "blur = cv.blur(img,(5,5))\n",
    "box = cv.boxFilter(img,-1, (3,3), normalize=False)\n",
    "titles = ['Original','Blurred','Box filtered']\n",
    "images = [img, blur, box]\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1),plt.imshow(images[i]),plt.title(titles[i])\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(211),plt.imshow(box),plt.title('Box filtered')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "#img = cv.add(img, cv.medianBlur(img,2))\n",
    "median_blur = cv.medianBlur(img,1)\n",
    "\n",
    "#f = plt.figure(figsize=(10,3))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(median_blur),plt.title('Median Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "\n",
    "#plt.imshow(img)\n",
    "cv.imshow('  ',median_blur)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "#bilateraled =  cv.bilateralFilter(img,9,175,175)\n",
    "#cv.imshow('  ',bilateraled)\n",
    "#cv.waitKey(0)\n",
    "#cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Morphological Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "We will see different functions like : cv.erode(), cv.dilate(), cv.morphologyEx() etc.\n",
    "\n",
    "It is normally performed on binary images. It needs two inputs, \n",
    "one is our original image, second one is called structuring element or kernel \n",
    "which decides the nature of operation.\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('You can also specify the size of kernel by the argument ksize. If ksize = -1, a 3x3 Scharr filter is used which gives better results than 3x3 Sobel filter.\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "erosion = cv.erode(img, kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(erosion),plt.title('Erored')\n",
    "#plt.imshow(erosion)\n",
    "# Rectangular Kernel\n",
    "rect_kernel = cv.getStructuringElement(cv.MORPH_RECT,(5,5))\n",
    "ellipse_kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))\n",
    "#kernel = np.ones((5,5), np.uint8)\n",
    "dilate_rect = cv.dilate(img, rect_kernel)\n",
    "dilate_ellip = cv.dilate(img, ellipse_kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(dilate_rect),plt.title('Dilated  using Rectangle kernel')\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(dilate_ellip),plt.title('Dilated using ellipse kernel')\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "morph_open = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(morph_open),plt.title('Morph open')\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "morph_close = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(morph_close),plt.title('Morph close')\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "morph_grade = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.subplot(122),plt.imshow(morph_grade),plt.title('Morph Gradient')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "OpenCV provides three types of gradient filters or\n",
    " High-pass filters, Sobel, Scharr and Laplacian.\n",
    "'''\n",
    "## Sobel\n",
    "'''\n",
    "You can also specify the size of kernel by the argument ksize. If ksize = -1, a 3x3 Scharr filter is used which gives better results than 3x3 Sobel filter.\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\sudoku.jpg',0)\n",
    "#gaus = cv.GaussianBlur(img, (5,5), 0)\n",
    "#plt.imshow(gaus, cmap='gray')\n",
    "\n",
    "lapcian = cv.Laplacian(img, cv.CV_64F)\n",
    "sobel_x = cv.Sobel(img, cv.CV_64F,1,0, ksize=5)\n",
    "sobel_y = cv.Sobel(img, cv.CV_64F,0,1, ksize=5)\n",
    "\n",
    "plt.subplot(2,2,1),plt.imshow(lapcian, cmap='gray'),plt.title('Lapcian')\n",
    "plt.subplot(2,2,2),plt.imshow(sobel_x, cmap='gray'),plt.title('Sobel_X')\n",
    "plt.subplot(2,2,3),plt.imshow(sobel_y, cmap='gray'),plt.title('Sobel_Y')\n",
    "plt.subplot(2,2,4),plt.imshow(img, cmap='gray'),plt.title('Original')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ******** CANNY EDGE DETECTION *******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Four stages to the algorithm:\n",
    "    1. Noise reduction using Low pass filter lieg gaussianBlur( (5,5))\n",
    "    2. finding intensity gradient using High pass filter like Sobel()\n",
    "    3. Non- maximum suppresion, using the gradient magnitude and direction, \n",
    "        a full scan of image is done to remove any unwanted pixels \n",
    "        which may not constitute the edge. \n",
    "        \n",
    "        ** Each pixel is check for a local maxima in its neighborhood in the\n",
    "            direction of gradient, it is is a local maxima, it is considered\n",
    "            for the next stage, otherwise suppresed to 0\n",
    "    4. **Hysteresis Thresholding** we need two threshold values, minVal and \n",
    "        maxVal. Any edges with intensity gradient more than maxVal are sure \n",
    "        to be edges and those below minVal are sure to be non-edges, \n",
    "        so discarded. Those who lie between these two thresholds are \n",
    "        classified edges or non-edges based on their connectivity. \n",
    "        If they are connected to \"sure-edge\" pixels, they are considered \n",
    "        to be part of edges. Otherwise, they are also discarded\n",
    "        \n",
    "'''\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "\n",
    "edges = cv.Canny(img,100,200)\n",
    "plt.subplot(121),plt.imshow(img,cmap='gray'), plt.title(\"Messi Original\")\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap='gray'), plt.title(\"edges using canny\")\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cv.imshow(\"Messi original\",img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "cv.imshow(\"edges using canny\",edges)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi_canny_edge.jpg',edges)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Image Pyramids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "set of the same image with different resolutions are calle image pyramids  \n",
    "There are two kinds of Image Pyramids. \n",
    "    1) Gaussian Pyramid and \n",
    "    2) Laplacian Pyramids\n",
    "'''\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "lower_reso = cv.pyrDown(img)\n",
    "print(lower_reso.shape)\n",
    "cv.imshow(\"Pyramid Down \", lower_reso)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "higher_reso = cv.pyrUp(img)\n",
    "print(higher_reso.shape)\n",
    "cv.imshow(\"Pyramid Up \", higher_reso)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "higher_reso2 = cv.pyrUp(higher_reso)\n",
    "print(higher_reso2.shape)\n",
    "cv.imshow(\"Pyramid Up 2 \", higher_reso2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 CONTOURS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1 CONTOURS - Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "You will see these functions : cv.findContours(), cv.drawContours()\n",
    "\n",
    "Contours can be explained simply as a curve joining all the continuous points \n",
    "(along the boundary), having same color or intensity. \n",
    "The contours are a useful tool for shape analysis and object detection and recognition.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv.threshold(imgray, 127, 255, 0)\n",
    "im2, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plt.imshow(im2, cmap='gray')\n",
    "cv.imshow(\"Messig contoured\", im2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "cv.drawContours(img, contours, -1,(0,255,0),2)\n",
    "cv.imshow(\"Draw contour\", img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# to draw an individual contour, say 4th contour:\n",
    "cv.drawContours(img, contours,3, (255,0,0),3)\n",
    "cv.imshow(\"Draw contour\", img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "cnt = contours[200]\n",
    "cv.drawContours(img, [cnt],0, (0,0,255),2)\n",
    "cv.imshow(\"Draw contour\", img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 4.7.2 Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "ret,thresh = cv.threshold(img,127,255,0)\n",
    "im2,contours,hierarchy = cv.findContours(thresh, 1, 2)\n",
    "cnt = contours[00]\n",
    "M = cv.moments(cnt)\n",
    "print( M )\n",
    "\n",
    "area = cv.contourArea(contours[100])\n",
    "print(area)\n",
    "perimeter = cv.arcLength(contours[100],False)\n",
    "print(perimeter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date 9-MAY-2018\n",
    "##### 4.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Contour : More Functions\n",
    "'''\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "star = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\star.jpg')\n",
    "star_gray = cv.cvtColor(star,cv.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv.threshold(star_gray,127, 200, 0)\n",
    "#cv.imshow('Start threshold', thresh)\n",
    "#cv.waitKey(0)\n",
    "#cv.destroyAllWindows()\n",
    "\n",
    "img, contours, hierarchy = cv.findContours(thresh ,2,1)\n",
    "cnt = contours[0]\n",
    "hull = cv.convexHull(cnt, returnPoints = False)\n",
    "defects = cv.convexityDefects(cnt,hull)\n",
    "\n",
    "for i in range(1):#defects.shape[0]):\n",
    "    s,e,f,d = defects[i,0]\n",
    "    start = tuple(cnt[s][0])\n",
    "    end = tuple(cnt[e][0])\n",
    "    far = tuple(cnt[f][0])\n",
    "    cv.line(star,start,end,[0,255,0],2)\n",
    "    cv.circle(star,far,5,[0,0,255],2)\n",
    "cv.imshow('img',star)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "dist = cv.pointPolygonTest(cnt,(50,50),True)\n",
    "print(dist)\n",
    "dist = cv.pointPolygonTest(cnt,(50,50),False)\n",
    "print(dist)\n",
    "\n",
    "'''\n",
    " a function cv.matchShapes() which enables us to compare two shapes, \n",
    " or two contours and returns a metric showing the similarity. \n",
    " The lower the result, the better match it is. It is calculated based on the \n",
    " hu-moment values\n",
    "'''\n",
    "\n",
    "star = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\star.jpg')\n",
    "star2 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\star2.png')\n",
    "\n",
    "star_gray = cv.cvtColor(star, cv.COLOR_BGR2GRAY)\n",
    "star2_gray = cv.cvtColor(star2, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "ret,thresh = cv.threshold(star_gray, 127,255,0)\n",
    "ret2,thresh2 = cv.threshold(star2_gray, 127,255,0)\n",
    "\n",
    "im2, contours, hierachy = cv.findContours(thresh, 2, 1)\n",
    "cnt = contours[0]\n",
    "im2, contours, hierachy = cv.findContours(thresh2, 2, 1)\n",
    "cnt2 = contours[0]\n",
    "\n",
    "ret = cv.matchShapes(cnt, cnt2,1,0.0)\n",
    "print(ret)\n",
    "\n",
    "cv.imshow('fd',cv.bitwise_and(im2,star2_gray, mask = star2_gray))\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.5  Contours Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "cv.findContours() returns the output, we got three arrays, \n",
    "first is the image, \n",
    "second is our contours, \n",
    "and one more output which we named as hierarchy \n",
    "'''\n",
    "cv.findContours()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 HISTOGRAMS \n",
    "#### 4.8.1  Histograms - 1 : Find, Plot, Analyze !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    " You can consider histogram as a graph or plot, which gives you an \n",
    " overall idea about the intensity distribution of an image. It is a plot \n",
    " with pixel values (ranging from 0 to 255, not always) in X-axis and \n",
    " corresponding number of pixels in the image on Y-axis\n",
    " \n",
    " By looking at the histogram of an image, you get intuition about contrast, \n",
    " brightness, intensity distribution etc of that image.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "#img = cv.imread('home.jpg',0)\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "plt.hist(img.ravel(),256,[0,256]); plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "#img = cv.imread('home.jpg')\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "color = ('b','g','r')\n",
    "for i,col in enumerate(color):\n",
    "    histr = cv.calcHist([img],[i],None,[256],[0,256])\n",
    "    plt.plot(histr,color = col)\n",
    "    plt.xlim([0,256])\n",
    "plt.show()\n",
    "\n",
    "#img = cv.imread('home.jpg',0)\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "# create a mask\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "mask[100:300, 100:400] = 255\n",
    "masked_img = cv.bitwise_and(img,img,mask = mask)\n",
    "# Calculate histogram with mask and without mask\n",
    "# Check third argument for mask\n",
    "hist_full = cv.calcHist([img],[0],None,[256],[0,256])\n",
    "hist_mask = cv.calcHist([img],[0],mask,[256],[0,256])\n",
    "plt.subplot(221), plt.imshow(img, 'gray')\n",
    "plt.subplot(222), plt.imshow(mask,'gray')\n",
    "plt.subplot(223), plt.imshow(masked_img, 'gray')\n",
    "plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)\n",
    "plt.xlim([0,256])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.2 Histogram Equalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "why :  normally improves the contrast of the image. \n",
    "The method is useful in images with backgrounds and foregrounds that are \n",
    "both bright or both dark. In particular, the method can lead to better views \n",
    "of bone structure in x-ray images, and to better detail in photographs that \n",
    "are over or under-exposed\n",
    "\n",
    "A disadvantage of the method is that it is indiscriminate. \n",
    "It may increase the contrast of background noise, while decreasing \n",
    "the usable signal.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Unequalized_Hawkes.jpg',0)\n",
    "hist,bins = np.histogram(img.flatten(),256,[0,256])\n",
    "cdf = hist.cumsum()\n",
    "cdf_normalized = cdf * float(hist.max()) / cdf.max()\n",
    "plt.plot(cdf_normalized, color = 'b')\n",
    "plt.hist(img.flatten(),256,[0,256], color = 'r')\n",
    "plt.xlim([0,256])\n",
    "plt.legend(('cdf','histogram'), loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    " we need a transformation function which maps the input pixels in \n",
    " brighter region to output pixels in full region.\n",
    "'''\n",
    "\n",
    "cdf_m = np.ma.masked_equal(cdf, 0)\n",
    "'''\n",
    " For masked array, all operations are performed on non-masked elements.\n",
    "'''\n",
    "cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max() - cdf_m.min())\n",
    "cdf = np.ma.filled(cdf_m,0).astype('uint8')\n",
    "img2 = cdf[img]\n",
    "plt.imshow(img2,cmap='gray')\n",
    "\n",
    "hist,bins = np.histogram(img2.flatten(),256,[0,256])\n",
    "cdf = hist.cumsum()\n",
    "cdf_normalized = cdf * float(hist.max()) / cdf.max()\n",
    "plt.plot(cdf_normalized, color = 'b')\n",
    "plt.hist(img2.flatten(),256,[0,256], color = 'r')\n",
    "plt.xlim([0,256])\n",
    "plt.legend(('cdf','histogram'), loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "# Histogram equalization in openCV using cv.equalizeHist()\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Unequalized_Hawkes.jpg',0)\n",
    "equ = cv.equalizeHist(img)\n",
    "res = np.hstack((img,equ)) #stacking images side-by-side\n",
    "plt.imshow(res,cmap='gray', shape=(15,15))\n",
    "'''\n",
    "Histogram equalization is good when histogram of the image is confined to a \n",
    "particular region. It won't work good in places where there is large \n",
    "intensity variations where histogram covers a large region, ie both bright \n",
    "and dark pixels are present.\n",
    "'''\n",
    "#cv.imwrite('res.png',res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLAHE (Contrast Limited Adaptive Histogram Equalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\tsikba.jpg',0)\n",
    "# create a CLAHE object (Arguments are optional).\n",
    "clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "cl1 = clahe.apply(img)\n",
    "plt.imshow(cl1,cmap='gray')\n",
    "plt.imshow(img,cmap='gray')\n",
    "#cv.imwrite('clahe_2.jpg',cl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date 10-MAY-2018\n",
    "#### 4.8.3   2D Histogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "calculated using the same function, cv.calcHist(). \n",
    "For color histograms, we need to convert the image from BGR to HSV. \n",
    "(Remember, for 1D histogram, we converted from BGR to Grayscale).\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "hist = cv.calcHist(img_hsv, [0,1], None, [180,255], [0,180,0,255])\n",
    "cv.imshow\n",
    "cv.imshow('yui[', hist)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "plt.imshow(hist,cmap='gray')\n",
    "plt.legend()\n",
    "\n",
    "## 2D Histogram in numpy\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "hist, xbins, ybins = np.histogram2d(hsv[...,0].ravel(), hsv[...,1].ravel(),[180,256], [[0,180],[0,256]])\n",
    "plt.imshow(hist, interpolation = 'nearest',cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4 Histogram Backprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "target = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "hsvt = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "# calculating object histogram\n",
    "roihist = cv.calcHist([hsv[20:80,220:270,]],[0, 1], None, [180, 256], [0, 180, 0, 256] )\n",
    "\n",
    "# normalize histogram and apply backprojection\n",
    "cv.normalize(roihist,roihist,0,255,cv.NORM_MINMAX)#, mask = hsv[60:80,200:240,])\n",
    "dst = cv.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)\n",
    "\n",
    "# Now convolute with circular disc\n",
    "disc = cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))\n",
    "cv.filter2D(dst,-1,disc,dst)\n",
    "\n",
    "# threshold and binary AND\n",
    "ret,thresh = cv.threshold(dst,20,255,0)\n",
    "thresh = cv.merge((thresh,thresh,thresh))\n",
    "res = cv.bitwise_and(target,thresh)\n",
    "\n",
    "res = np.vstack((target,thresh,res))\n",
    "cv.imwrite('res2.jpg',res)\n",
    "\n",
    "#hsv[60:80,200:240,]\n",
    "cv.rectangle(img, (20,220), (80,270),(0,0,255), 0)\n",
    "cv.imshow('fdfd',disc)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "`'''\n",
    "WHY :   is used to analyze the frequency characteristics of various filters. \n",
    "For images, 2D Discrete Fourier Transform (DFT) is used to find \n",
    "the frequency domain.\n",
    "\n",
    "You can consider an image as a signal which is sampled in two directions. \n",
    "So taking fourier transform in both X and Y directions gives you the frequency\n",
    "representation of image.\n",
    "\n",
    "So we can say, edges and noises are high frequency contents in an image. If \n",
    "there is no much changes in amplitude, it is a low frequency component. \n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "# Find the fouriest transfprm \n",
    "f = np.fft.fft2(img)\n",
    "#zero frequency component (DC component) will be at top left corner. \n",
    "#If you want to bring it to center, \n",
    "#you need to shift the result by N2 in both the directions\n",
    "fshift = np.fft.fftshift(f)\n",
    "magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img,cmap='gray'), plt.title('Input Image')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(magnitude_spectrum, cmap='gray'), plt.title('Magnitude Spectrum')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Now you can do some operations in frequency domain, \n",
    "like high pass filtering and reconstruct the image, ie find inverse DFT. \n",
    "For that you simply remove the low frequencies by masking with a \n",
    "rectangular window of size 60x60. \n",
    "Then apply the inverse shift using np.fft.ifftshift() \n",
    "so that DC component again come at the top-left corner. \n",
    "Then find inverse FFT using np.ifft2() function. \n",
    "The result, again, will be a complex number. \n",
    "You can take its absolute value.\n",
    "'''\n",
    "\n",
    "rows, cols = img.shape\n",
    "crow, ccol = rows//2, cols//2  \n",
    "fshift[crow-30 : crow+30, ccol-30:ccol+30] = 0 #It is a mask\n",
    "\n",
    "f_ishift = np.fft.ifftshift(fshift)\n",
    "img_back = np.fft.ifft2(f_ishift)\n",
    "img_back = np.abs(img_back)\n",
    "\n",
    "plt.subplot(111), plt.imshow(img_back,cmap='gray'),plt.title('Image after HPf')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(111), plt.imshow(img_back),plt.title('result in JET')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "## Fourier Transformation in OpenCV cv.dft() cv.idft()\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "\n",
    "dft = cv.dft(np.float32(img), flags= cv.DFT_COMPLEX_OUTPUT)\n",
    "dft_shift = np.fft.fftshift(dft)\n",
    "\n",
    "magnitude_spectrum = 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,-1]))\n",
    "plt.subplot(121),plt.imshow(img,cmap='gray'), plt.title('Input Image')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(magnitude_spectrum, cmap='gray'), plt.title('Magnitude Spectrum')\n",
    "plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "rows, cols = img.shape\n",
    "crow,ccol = rows//2 , cols//2\n",
    "# create a mask first, center square is 1, remaining all zeros\n",
    "mask = np.zeros((rows,cols,2),np.uint8)\n",
    "mask[crow-30:crow+30, ccol-30:ccol+30] = 1\n",
    "#mask = cv.getGaussianKernel(*dft_shift.shape)\n",
    "# apply mask and inverse DFT\n",
    "fshift = dft_shift*mask\n",
    "f_ishift = np.fft.ifftshift(fshift)\n",
    "img_back = cv.idft(f_ishift)\n",
    "img_back = cv.magnitude(img_back[:,:,0],img_back[:,:,1])\n",
    "plt.subplot(121),plt.imshow(img, cmap = 'gray')\n",
    "plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(img_back, cmap = 'gray')\n",
    "plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9 Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg',0)\n",
    "img2 = img.copy()\n",
    "template = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi_face.jpg',0)\n",
    "w, h = template.shape[::-1]\n",
    "# All the 6 methods for comparison in a list\n",
    "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',\n",
    "            'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']\n",
    "for meth in methods:\n",
    "    img = img2.copy()\n",
    "    method = eval(meth)\n",
    "    # Apply template Matching\n",
    "    res = cv.matchTemplate(img,template,method)\n",
    "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n",
    "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED, cv.TM_CCORR]:\n",
    "        top_left = min_loc\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "    cv.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "    plt.subplot(121),plt.imshow(res,cmap = 'gray')\n",
    "    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122),plt.imshow(img,cmap = 'gray')\n",
    "    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "    plt.suptitle(meth)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template Matching with Multiple Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In the previous section, we searched image for Messi's face, \n",
    "which occurs only once in the image. Suppose you are searching for an object \n",
    "which has multiple occurrences, cv.minMaxLoc() won't give you all the locations\n",
    ". In that case, we will use thresholding\n",
    "'''\n",
    "    \n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img_rgb = cv.imread('mario.png')\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)\n",
    "template = cv.imread('mario_coin.png',0)\n",
    "w, h = template.shape[::-1]\n",
    "res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)\n",
    "threshold = 0.8\n",
    "loc = np.where( res >= threshold)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n",
    "cv.imwrite('res.png',img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date 11 May 2018\n",
    "## Hough Line transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Hought transform is a technique to detect any shape, if that shape can be \n",
    "represented in a mathematical form.\n",
    "\n",
    "Any line can be represented in these two terms, (ρ,θ). \n",
    "So first it creates a 2D array or accumulator \n",
    "(to hold the values of the two parameters) and it is set to 0 initially.\n",
    "\n",
    " cv.HoughLines(). It simply returns an array of :math:(rho, theta)` values. \n",
    " ρ is measured in pixels and θ is measured in radians. First parameter, \n",
    " Input image should be a binary image, so apply threshold or use canny edge \n",
    " detection before applying hough transform. \n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\sudoku.jpg')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "edges = cv.Canny(gray, 50, 150, apertureSize = 3)\n",
    "\n",
    "lines = cv.HoughLines(edges, 1, np.pi/180, 70)\n",
    "for line in lines:\n",
    "#    print(line)\n",
    "    rho, theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "    \n",
    "    cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\houghlines3.jpg',img)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\sudoku.jpg')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "edges = cv.Canny(gray, 50 ,150, apertureSize =3)\n",
    "lines = cv.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength = 10, maxLineGap = 1)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "    \n",
    "cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\houghlines5.jpg',img)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Hough circle transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\opencv_logo.png')\n",
    "img = cv.medianBlur(img, 5)\n",
    "cimg = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "circles = cv.HoughCircles(cimg, cv.HOUGH_GRADIENT,1,20,param1=50,param2=30,\n",
    "                          minRadius=0,maxRadius=0)\n",
    "\n",
    "circles = np.uint16(np.around(circles))\n",
    "for i in circles[0,:]:\n",
    "#    print(i[0], i[1], i[2])\n",
    "    cv.circle(cimg,(i[0],i[1]), i[2],(0,255,0),-1)\n",
    "    cv.circle(cimg,(i[0],i[1]), i[2],(0,0,255),3)\n",
    "    \n",
    "cv.imshow('detected circles',cimg)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 Watershed Algorithm -> marker-based watershed algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "What we do is to give different labels for our object we know. \n",
    "Label the region which we are sure of being the foreground or object with one \n",
    "color (or intensity), \n",
    "label the region which we are sure of being background or non-object with \n",
    "another color and \n",
    "finally the region which we are not sure of anything, \n",
    "label it with 0. \n",
    "That is our marker. \n",
    "Then apply watershed algorithm. \n",
    "Then our marker will be updated with the labels we gave, and the boundaries \n",
    "of objects will have a value of -1\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\water_coins.jpg')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)\n",
    "\n",
    "# noise removal\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv.dilate(opening,kernel,iterations=3)\n",
    "\n",
    "# Finding sure foreground area\n",
    "dist_transform = cv.distanceTransform(opening,cv.DIST_L2,5)\n",
    "ret, sure_fg = cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    "\n",
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv.subtract(sure_bg,sure_fg)\n",
    "# Marker labelling\n",
    "ret, markers = cv.connectedComponents(sure_fg)\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers+1\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown==255] = 0\n",
    "markers = cv.watershed(img,markers)\n",
    "img[markers == -1] = [0,0,255]\n",
    "\n",
    "cv.imshow('Water coins threshold',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 Interactive Foreground Extraction using GrabCut Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi.jpg')\n",
    "mask = np.zeros(img.shape[:2],np.uint8)\n",
    "\n",
    "bgdmodel = np.zeros((1,65), np.float64)\n",
    "fgdmodel = np.zeros((1,65), np.float64)\n",
    "rectangle = (50,50,450,290)\n",
    "# this will modify the mask with the help ot that mask we will grab the \n",
    "# foreground\n",
    "cv.grabCut(img,mask,rectangle,bgdmodel,fgdmodel,5,cv.GC_INIT_WITH_RECT)\n",
    "\n",
    "mask2 = np.where((mask == 2) | (mask ==0), 0, 1).astype('uint8')\n",
    "img = img*mask2[:,:,np.newaxis]\n",
    "#plt.imshow(img)\n",
    "\n",
    "new_mask = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\flowers\\\\messi_mask.jpg',0)\n",
    "mask[new_mask ==0] =0\n",
    "mask[new_mask ==255] =1\n",
    "mask , bgdmodle,fdgmodel = cv.grabCut(img,mask,None,bgdmodel,fgdmodel,5,cv.GC_INIT_WITH_MASK)\n",
    "mask2 = np.where((mask == 2) | (mask ==0), 0, 1).astype('uint8')\n",
    "img = img*mask2[:,:,np.newaxis]\n",
    "plt.imshow(img)\n",
    "\n",
    "# Now we have to manually mark the areas to filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V FEATURE DETECTION AND DESCRIPTIONS\n",
    "###  Understanding Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "How can you stitch a lot of natural images to a single image?\n",
    "\n",
    "The answer is, we are looking for specific patterns or specific features \n",
    "which are unique, can be easily tracked and can be easily compared. \n",
    "If we go for a definition of such a feature, we may find it difficult to \n",
    "express it in words, but we know what they are. If someone asks you to \n",
    "point out one good feature which can be compared across several images, \n",
    "you can point out one. That is why even small children can simply play these(jigsaw) \n",
    "games. We search for these features in an image, find them, look for the same \n",
    "features in other images and align them. That's it. \n",
    "(In jigsaw puzzle, we look more into continuity of different images). \n",
    "All these abilities are present in us inherently.\n",
    "\n",
    "So our one basic question expands to more in number, but becomes more specific.\n",
    "What are these features?. (The answer should be understandable also to a computer.)\n",
    "\n",
    "So basically, corners are considered to be good features in an image. \n",
    "(Not just corners, in some cases blobs are considered good features).\n",
    "\n",
    "So now we answered our question, \"what are these features?\". \n",
    "But next question arises. How do we find them? Or how do we find the corners?. \n",
    "We answered that in an intuitive way, i.e., look for the regions in images \n",
    "which have maximum variation when moved (by a small amount) in all regions \n",
    "around it. This would be projected into computer language in coming chapters. \n",
    "So finding these image features is called \"Feature Detection\".\n",
    "\n",
    "So in this module, we are looking to different algorithms in OpenCV \n",
    "to find features, describe them, match them etc.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harris Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We will see the functions: cv.cornerHarris(), cv.cornerSubPix()\n",
    "\n",
    "Basically finds the difference in intensity for a displacement of (u,v) \n",
    "in all directions.\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#cv.drawChessboardCorners()\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Chess_Board.png')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "\n",
    "dst = cv.cornerHarris(gray, 2,3,0.1)\n",
    "\n",
    "#result is dilated for making the  corners, not important\n",
    "dst = cv.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "cv.imshow('this',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "'''\n",
    "We may need to find the corners with maximum accuracy\n",
    "\n",
    "cv.cornerSubPix()\n",
    "\n",
    "As usual, we need to find the harris corners first. \n",
    "Then we pass the centroids of these corners \n",
    "(There may be a bunch of pixels at a corner, we take their centroid) \n",
    "to refine them\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\Chess_Board.png')\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# find Harris corners\n",
    "gray = np.float32(gray)\n",
    "dst = cv.cornerHarris(gray,2,31,0.04)\n",
    "dst = cv.dilate(dst,None)\n",
    "ret, dst = cv.threshold(dst,0.01*dst.max(),255,0)\n",
    "dst = np.uint8(dst)\n",
    "\n",
    "# find centroids\n",
    "ret, labels, stats, centroids = cv.connectedComponentsWithStats(dst)\n",
    "\n",
    "#define the criteria to stop and refine the cornere\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv.cornerSubPix(gray,np.float32(centroids),(5,5),(-1,-1),criteria)\n",
    "\n",
    "res = np.hstack((centroids,corners))\n",
    "res = np.int0(res)\n",
    "\n",
    "img[res[:,1],res[:,0]] = [0,0,255]\n",
    "img[res[:,3],res[:,2]] = [0,255,0]\n",
    "\n",
    "cv.imshow('subPix()', img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "OpenCV has a function, cv.goodFeaturesToTrack(). It finds N strongest corners \n",
    "in the image by Shi-Tomasi method (or Harris Corner Detection, if you specify it)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\BLOX2.jpg')\n",
    "#print(img.shape)\n",
    "#img = cv.resize(img, -1, fx=2, fy=2)\n",
    "#print(img.shape)\n",
    "\n",
    "\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "corners = cv.goodFeaturesToTrack(gray, 100,0.01,30)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv.circle(img, (x,y),3, 255,-1)\n",
    "cv.imshow('cv.goodFeaturesToTrack()', img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIFT (Scale-Invariant Feature Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('home.jpg')\n",
    "gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "#sift.detect() function finds the keypoint in the images. \n",
    "#You can pass a mask if you want to search only a part of image. \n",
    "#Each keypoint is a special structure which has many attributes like its (x,y) \n",
    "#coordinates, size of the meaningful neighbourhood, angle which specifies its \n",
    "#orientation, response that specifies strength of keypoints etc.\n",
    "kp = sift.detect(gray, None)\n",
    "\n",
    "img = cv.drawKeypoints(gray, kp, img)\n",
    "\n",
    "img=cv.drawKeypoints(gray,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv.imwrite('sift_keypoints.jpg',img)\n",
    "\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "kp, des = sift.detectAndCompute(gray,None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date 15 may 2018\n",
    "#### Speeded-Up Robust Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In SIFT, Lowe approximated Laplacian of Gaussian with Difference of Gaussian \n",
    "for finding scale-space. SURF goes a little further and approximates LoG with \n",
    "\"Box Filter\"\n",
    "\n",
    "One big advantage of this approximation is that, convolution with box filter \n",
    "can be easily calculated with the help of integral images.\n",
    "\n",
    "SURF is good at handling images with blurring and rotation, \n",
    "but not good at handling viewpoint change and illumination change.\n",
    "\n",
    "SURF is more like a blob detector\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\jaguar.jpg')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SURF object. You can specify params here or later.\n",
    "# Here I set Hessian Threshold to 3000\n",
    "surf = cv.xfeatures2d.SURF_create(3000)\n",
    "\n",
    "# Find keypoints and descriptors directly\n",
    "kp, des = surf.detectAndCompute(img,None)\n",
    "print(len(kp))\n",
    "\n",
    "img2 = cv.drawKeypoints(img, kp, None, (0,0,255), 2)\n",
    "#plt.imshow(img2)\n",
    "\n",
    "#cv.imwrite('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\jaguar_SURF.jpg', img2)\n",
    "cv.imshow('Jaguar',img2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "## Now apply the U-SURF, SO That it won't find orientation\n",
    "# Check upright flag, if it False, set it to True\n",
    "print(surf.getUpright())\n",
    "\n",
    "surf.setUpright(True)\n",
    "# Recompute the feature points and draw it\n",
    "'''\n",
    "All the orientations are shown in same direction. \n",
    "It is faster than previous. If you are working on cases where orientation is \n",
    "not a problem (like panorama stitching) etc, this is better.\n",
    "'''\n",
    "kp = surf.detect(img,None)\n",
    "img2 = cv.drawKeypoints(img,kp,None,(255,0,0),2)\n",
    "cv.imshow('Jaguar',img2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST algortihm for corner detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\BLOX2.jpg')\n",
    "# Initiate FAST object with default values\n",
    "fast = cv.FastFeatureDetector_create()\n",
    "\n",
    "# find and draw the keypoints\n",
    "kp = fast.detect(img,None)\n",
    "img2 = cv.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "# Print all default params\n",
    "print( \"Threshold: {}\".format(fast.getThreshold()) )\n",
    "print( \"nonmaxSuppression:{}\".format(fast.getNonmaxSuppression()) )\n",
    "print( \"neighborhood: {}\".format(fast.getType()) )\n",
    "print( \"Total Keypoints with nonmaxSuppression: {}\".format(len(kp)) )\n",
    "#cv.imwrite('fast_true.png',img2)\n",
    "cv.imshow('FAST algorithm',img2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Disable nonmaxSuppression\n",
    "fast.setNonmaxSuppression(0)\n",
    "kp = fast.detect(img,None)\n",
    "print( \"Total Keypoints without nonmaxSuppression: {}\".format(len(kp)) )\n",
    "img3 = cv.drawKeypoints(img, kp, None, color=(0,0,255))\n",
    "#cv.imwrite('fast_false.png',img3)\n",
    "cv.imshow('FAST_WITHout_nonmaxSuppression',img3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRIEF (Binary Robust Independent Elementary Features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "BRIEF is a feature descriptor, it doesn't provide any method to find the \n",
    "features. So you will have to use any other feature detectors like SIFT, SURF \n",
    "etc. The paper recommends to use CenSurE which is a fast detector and BRIEF \n",
    "works even slightly better for CenSurE points than for SURF points.\n",
    "\n",
    "In short, BRIEF is a faster method feature descriptor calculation and matching.\n",
    "It also provides high recognition rate unless there is large in-plane rotation.\n",
    "'''\n",
    "# bELOW is the example of brief of feature descriptor with censure feature\n",
    "# detector, cenSure detector is called \"star\" detector in openCV\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\BLOX2.jpg')\n",
    "\n",
    "# Initiate FAST detector\n",
    "star = cv.xfeatures2d.StarDetector_create()\n",
    "#Initiate BRIEF extractor\n",
    "brief = cv.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "# find the keypoints with STAR\n",
    "kp = star.detect(img,None)\n",
    "\n",
    "#compute the descriptors wiht BRIEF\n",
    "kp, des = brief.compute(img, kp)\n",
    "\n",
    "print(brief.descriptorSize())\n",
    "print(des.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB (Oriented FAST and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ORB is basically a fusion of FAST keypoint detector and BRIEF descriptor with \n",
    "many modifications to enhance the performance. \n",
    "First it use FAST to find keypoints, \n",
    "then apply Harris corner measure to find top N points among them. \n",
    "It also use pyramid to produce multiscale-features.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\BLOX2.jpg')\n",
    "# Initiate ORB detector\n",
    "orb = cv.ORB_create()\n",
    "# find the keypoints with ORB\n",
    "kp = orb.detect(img,None)\n",
    "\n",
    "# compute the descriptors with ORB\n",
    "kp, des = orb.compute(img,kp)\n",
    "\n",
    "img2 = cv.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\n",
    "\n",
    "cv.imshow('ORB detector',img2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching\n",
    "#### -- Brute-Force Matcher with ORB descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img1 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box.png')\n",
    "img2 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box_in_scene.png')\n",
    "# Initiate ORB detector\n",
    "orb = cv.ORB_create()\n",
    "\n",
    "# find the keypoints and descriptoros with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "#create the BFMatcher object\n",
    "bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck = True)\n",
    "\n",
    "#Match descriptors\n",
    "matches = bf.match(des1, des2)\n",
    "# sor them in the order of the distance , less is better match\n",
    "matches = sorted(matches, key = lambda x: x.distance)\n",
    "# Draw first 10 matches\n",
    "img3 = cv.drawMatches(img1, kp1, img2, kp2, matches[:100],None, flags=2)\n",
    "cv.imshow('BF MATCHER', img3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Brute-Force Matching with SIFT Descriptors and Ratio Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img1 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box.png') # queryImage\n",
    "img2 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box_in_scene.png') # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "#sift = cv.SIFT()\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])\n",
    "# cv.drawMatchesKnn expects list of lists as matches.\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=2)\n",
    "plt.imshow(img3),plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date 16-May-2018\n",
    "#### FLANN based Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "FLANN stands for Fast Library for Approximate Nearest Neighbors. \n",
    "It contains a collection of algorithms optimized for fast nearest neighbor \n",
    "search in large datasets and for high dimensional features. \n",
    "It works faster than BFMatcher for large datasets.\n",
    "\n",
    "For FLANN based matcher, we need to pass two dictionaries which specifies the \n",
    "algorithm to be used, its related parameters etc. First one is IndexParams. \n",
    "\n",
    "Second dictionary is the SearchParams. It specifies the number of times the \n",
    "trees in the index should be recursively traversed. Higher values gives better \n",
    "precision, but also takes more time. If you want to change the value, pass \n",
    "search_params = dict(checks=100).\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img1 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box.png') # queryImage\n",
    "img2 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box_in_scene.png') # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "#sift = cv.SIFT()\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "## FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, tree = 5)\n",
    "search_params = dict(check=50) # or pass empty dictionary\n",
    "\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, k =2)\n",
    "\n",
    "#Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's Paper -> SIFT descriptor founder\n",
    "for i , (m,n) in enumerate(matches):\n",
    "    if m.distance < 0.75* n.distance:\n",
    "        matchesMask[i] = [1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor= (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = 0)\n",
    "\n",
    "img3 = cv.drawMatchesKnn(img1, kp1, img2, kp2, matches, None,**draw_params)\n",
    "cv.imshow('cv.FlannBasedMatcher()',img3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img1 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box.png') # queryImage\n",
    "img2 = cv.imread('C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\box_in_scene.png') # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "## FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, tree = 5)\n",
    "search_params = dict(check=50) # or pass empty dictionary\n",
    "\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k =2)\n",
    "\n",
    "good =[]\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "MIN_MATCH_COUNT = 10\n",
    "if len(matches )> MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt  for m in good]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt  for m in good]).reshape(-1,1,2)\n",
    "    \n",
    "    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    \n",
    "    h,w,d = img1.shape\n",
    "    pts = np.float32([[0,0],[0, h-1],[w-1, h-1],[w-1, 0]]).reshape(-1,1,2)\n",
    "    dst = cv.perspectiveTransform(pts, M)\n",
    "    \n",
    "    img2 = cv.polylines(img2, [np.int32(dst)],True,255,3,cv.LINE_AA)\n",
    "    \n",
    "else:\n",
    "    print('Not enouh matche are found - {}/{}'.format(len(good), MIN_MATCH_COUNT))\n",
    "    matchesMask = None\n",
    "    \n",
    "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor= (0,0,255),\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "img3 = cv.drawMatches(img1, kp1, img2, kp2, good, None,**draw_params)\n",
    "cv.imshow('cv.FlannBasedMatcher()',img3)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI Video Analysis\n",
    "### 6.1 Meanshift and Camshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n",
      "(400, 250, 125, 90)\n"
     ]
    }
   ],
   "source": [
    "#Meanshift\n",
    "'''\n",
    "We normally pass the histogram backprojected image and initial target location. \n",
    "When the object moves, obviously the movement is reflected in histogram backprojected image. \n",
    "As a result, meanshift algorithm moves our window to the new location with maximum density\n",
    "'''\n",
    "# To use meanshift in OpenCV, first we need to setup the target, find its histogram so that we can \n",
    "# backproject the target on each frame for calculation of meanshift. We also need to provide initial \n",
    "# location of window. For histogram, only Hue is considered here. Also, to avoid false values due to \n",
    "# low light, low light values are discarded using cv.inRange() function.\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture('./data/slow.flv')\n",
    "\n",
    "#Read the first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "#Set up the initila location of the window\n",
    "r,h,c,w = 250,90,350,125  # simply hardcoded the values\n",
    "track_window = (c,r,w,h)\n",
    "\n",
    "#SET up the ROI for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "\n",
    "#Mask for filtering the low light\n",
    "mask = cv.inRange(hsv_roi, np.array((0.,60.,32.)), np.array((180.,255.,255.)))\n",
    "# only Hue is consider for calculation of Histogram\n",
    "roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "cv.normalize(roi_hist,roi_hist, 0, 255, cv.NORM_MINMAX)\n",
    "\n",
    "#Set up the termination criteria, either 10 iteration or move atleat 1 pt\n",
    "term_criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10,1)\n",
    "\n",
    "while(1):\n",
    "    ret ,frame = cap.read()\n",
    "    if ret == True:\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        \n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.meanShift(dst,track_window ,term_criteria)\n",
    "        \n",
    "        #Draw it on image\n",
    "        x,y,w,h = track_window\n",
    "        print(track_window)\n",
    "        img2 = cv.rectangle(frame,(x,y),(x+w,y+h),255,2)\n",
    "        cv.imshow('imgage', img2)\n",
    "        \n",
    "        k = cv.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        else:\n",
    "#             print('i ma here')\n",
    "            cv.imwrite(\"./data/generated/\"+chr(k)+\".jpg\",img2)\n",
    "    else:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# !pwd\n",
    "criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10,1)\n",
    "print(criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## camShift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(path + 'slow.flv')\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "# setup initial location of window\n",
    "r,h,c,w = 250,90,300,125  # simply hardcoded the values\n",
    "track_window = (c,r,w,h)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "hsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\n",
    "roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n",
    "# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "while(1):\n",
    "    ret ,frame = cap.read()\n",
    "    if ret == True:\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.CamShift(dst, track_window, term_crit)\n",
    "        # Draw it on image\n",
    "        pts = cv.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv.polylines(frame,[pts],True, 255,2)\n",
    "        cv.imshow('img2',img2)\n",
    "        k = cv.waitKey(60) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        else:\n",
    "            cv.imwrite(path+'generated\\\\cam_'+chr(k)+\".jpg\",img2)\n",
    "    else:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Optical Flow with lucas-kanade method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "we will be using  cv.calcOpticalFlowPyrLK() \n",
    "\n",
    "Optical flow is the pattern of apparent motion of image objects between two \n",
    "consecutive frames caused by the movemement of object or camera. \n",
    "It is 2D vector field where each vector is a displacement vector showing the \n",
    "movement of points from first frame to second.\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(path+'slow.flv')\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0,255,(100,3))\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "p0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret,frame = cap.read()\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    good_new = p1[st==1]\n",
    "    good_old = p0[st==1]\n",
    "    # draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "        a,b = new.ravel()\n",
    "        c,d = old.ravel()\n",
    "        mask = cv.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1)\n",
    "    img = cv.add(frame,mask)\n",
    "    cv.imshow('frame',img)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1,1,2)\n",
    "cv.destroyAllWindows()\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Optical Flow in OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lucas-Kanade method computes optical flow for a sparse feature set \n",
    "(in our example, corners detected using Shi-Tomasi algorithm)\n",
    "\n",
    "Below sample shows how to find the dense optical flow using above algorithm. \n",
    "We get a 2-channel array with optical flow vectors, (u,v). \n",
    "We find their magnitude and direction. We color code the result for better \n",
    "visualization. Direction corresponds to Hue value of the image. \n",
    "Magnitude corresponds to Value plane.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "path = 'C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\video\\\\'\n",
    "cap = cv.VideoCapture(path+'vtest.avi')\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[...,1] = 255\n",
    "\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    next_frame = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    mag, ang = cv.cartToPolar(flow[...,0],flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv.normalize(mag,None,0,255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    \n",
    "    cv.imshow('Black', bgr)\n",
    "    cv.imshow('New image', frame2)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    \n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv.imwrite('opticalfb.png', frame2)\n",
    "        cv.imwrite('opticalhsv.png', bgr)\n",
    "    prv = next_frame\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  cv.createBackgroundSubtractorMOG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "It uses a method to model each background pixel by a mixture of K Gaussian \n",
    "distributions (K = 3 to 5). The weights of the mixture represent the time \n",
    "proportions that those colours stay in the scene. The probable background \n",
    "colours are the ones which stay longer and more static.\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "path = 'C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\video\\\\'\n",
    "cap = cv.VideoCapture(path+'vtest.avi')\n",
    "fgbg = cv.bgsegm.createBackgroundSubtractorMOG()\n",
    "\n",
    "while(1):\n",
    "    ret,frame = cap.read()\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    cv.imshow('Extracted FG',fgmask)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####  cv.createBackgroundSubtractorMOG2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(path+'vtest.avi')\n",
    "fgbg = cv.createBackgroundSubtractorMOG2()\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv.imshow('frame',fgmask)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cv.bgsegm.createBackgroundSubtractorGMG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(path+'vtest.avi')\n",
    "kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (3,3))\n",
    "fgbg = cv.bgsegm.createBackgroundSubtractorGMG()\n",
    "\n",
    "while(1):\n",
    "    ref, frame = cap.read()\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv.morphologyEx(fgmask, cv.MORPH_OPEN,kernel)\n",
    "    cv.imshow('frame',fgmask)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Date 18-May-2018\n",
    "## Camera Calibration\n",
    "'''\n",
    "RADIAL distortion, straight lines will appear curved. \n",
    "Its effect is more as we move away from the center of image\n",
    "\n",
    "TANGENTIAL distortion which occurs because image taking lense is not aligned \n",
    "perfectly parallel to the imaging plane.  So some areas in image may look nearer than expected.\n",
    "\n",
    "Intrinsic parameters are specific to a camera. \n",
    "It includes information like focal length ( fx,fy), optical centers ( cx,cy) etc.\n",
    "\n",
    "Extrinsic parameters corresponds to rotation and translation vectors which \n",
    "translates a coordinates of a 3D point to a coordinate system.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "\n",
    "path = 'C:\\\\Users\\\\amitkumar_kataria\\\\Desktop\\\\CV\\\\Data\\\\'\n",
    "#termination criteris\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "#prepare object points, like (0,0,0), (1,0,0),(2,0,0).....(6,5,0)\n",
    "objp = np.zeros((6*7,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
    "\n",
    "#Arrays to store object points and image points from allthe images\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane\n",
    "\n",
    "images = glob.glob(path+'camera_calibration\\\\'+'*.jpg')\n",
    "\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (7,6),None)\n",
    "    \n",
    "    # If found, add object points and image points (after refiing them)\n",
    "    print(ret, fname)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1), criteria)\n",
    "        imgpoints.append(corners)\n",
    "         \n",
    "        #Draw and display the corners\n",
    "        cv.drawChessboardCorners(img,(7,6),corners, ret)\n",
    "        cv.imshow('chess board with detected corners', img)\n",
    "        cv.waitKey(0)\n",
    "         \n",
    "cv.destroyAllWindows()\n",
    "\n",
    "'''\n",
    "So now we have our object points and image points we are ready to go for \n",
    "calibration. For that we use the function, cv.calibrateCamera(). \n",
    "\n",
    "It returns the camera matrix, distortion coefficients, rotation and translation\n",
    "vectors\n",
    "'''\n",
    "ret, mtx, dist, rvecs, trvecs = cv.calibrateCamera(objpoints,imgpoints,\n",
    "                                                   gray.shape[::-1],None, None)\n",
    "\n",
    "'''\n",
    "we can refine the camera matrix based on a free scaling parameter using \n",
    "cv.getOptimalNewCameraMatrix(). \n",
    "If the scaling parameter alpha=0, it returns undistorted image with minimum \n",
    "unwanted pixels. \n",
    "So it may even remove some pixels at image corners. \n",
    "If alpha=1, all pixels are retained with some extra black images.     \n",
    "'''\n",
    "\n",
    "img = cv.imread(path+'camera_calibration\\\\'+'left12.jpg')\n",
    "h, w = img.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h),1, (w,h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we try to undistory an image\n",
    "#undistort   Using cv.undistort()\n",
    "dst = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "#crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "cv.imwrite(path+'camera_calibration\\\\'+'calibresult.png', dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using remapping \n",
    "'''\n",
    "This is curved path. First find a mapping function from distorted image to \n",
    "undistorted image.\n",
    "'''\n",
    "#undistort\n",
    "mapx, mapy = cv.initUndistortRectifyMap(mtx,dist, None, newcameramtx, (w,h),5)\n",
    "dst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)\n",
    "\n",
    "# crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "cv.imwrite(path+'camera_calibration\\\\'+'calibMAPresult.png', dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we save the intrinsic details like mtx, dist etc\n",
    "np.savez(path+'camera_calibration\\\\'+'calibresult.txt', *[mtx, dist, rvecs, trvecs])\n",
    "mtx1 = np.load(path+'camera_calibration\\\\'+'calibresult.txt.npz')\n",
    "mtx1['mtx']\n",
    "\n",
    "#Reprojection Error\n",
    "mean_error = 0\n",
    "for  i in range(len(objpoints)):\n",
    "    imgpoint2, _ = cv.projectPoints(objpoints[i],rvecs[i], trvecs[i],mtx,dist)\n",
    "    error = cv.norm(imgpoints[i], imgpoint2, cv.NORM_L2)/len(imgpoint2)\n",
    "    mean_error += error\n",
    "\n",
    "print( \"total error: {}\".format(mean_error/len(objpoints)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Machine Learning\n",
    "#### Understanding K-Nearest Neigbhbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0d67cf5c88>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature set containing (x,y) values of 25 known/training dat\n",
    "trainData = np.random.randint(0,100,(25,2)).astype(np.float32) \n",
    "# Labels each one either Red or Blue with numbers 0 and 1\n",
    "responses = np.random.randint(0,2,(25,1)).astype(np.float32)\n",
    "# Take Red families and plot them\n",
    "red = trainData[responses.ravel()==0]\n",
    "plt.scatter(red[:,0],red[:,1],80,'r','^')\n",
    "# Take Blue families and plot them\n",
    "blue = trainData[responses.ravel()==1]\n",
    "plt.scatter(blue[:,0],blue[:,1],80,'b','s')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  [[ 1.]] \n",
      "\n",
      "neighbours:  [[ 1.  1.  0.]] \n",
      "\n",
      "distance:  [[   26.   218.  1145.]]\n"
     ]
    }
   ],
   "source": [
    "newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)\n",
    "plt.scatter(newcomer[:,0],newcomer[:,1],80,'g','o')\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(trainData,cv.ml.ROW_SAMPLE,responses)#,respo\n",
    "ret, results, neighbours ,dist = knn.findNearest(newcomer, 3)\n",
    "print( \"result: \", results,\"\\n\")\n",
    "print( \"neighbours: \", neighbours,\"\\n\")\n",
    "print( \"distance: \", dist)\n",
    "# plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJBXq/wB/Ql0WLV0IagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d67ee6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10 new comers\n",
    "newcomers = np.random.randint(0,100,(10,2)).astype(np.float32)\n",
    "ret, results,neighbours,dist = knn.findNearest(newcomer, 3)\n",
    "# The results also will contain 10 labels.\n",
    "plt.scatter(newcomer[:,0],newcomer[:,1],80,'b','o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
